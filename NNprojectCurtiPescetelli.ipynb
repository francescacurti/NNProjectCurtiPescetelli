{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DESCRIPTION OF THE IMPLEMENTED METHOD**\n",
        "\n",
        "This notebook provides a reimplementation of the MaskTune method presented in the paper \"MaskTune: Mitigating Spurious Correlations by Forcing to Explore\" (https://arxiv.org/abs/2210.00055). MaskTune is a technique designed to mitigate the impact of spurious correlations in deep learning models. Spurious correlations are unintended associations between input features and target variables that can lead models to make incorrect predictions, especially when these correlations do not hold in new or shifted domains. Traditional methods to address this issue often require supervision or annotation of spurious features, which can be impractical. MaskTune addresses this by masking features that a model has learned to rely on during training, forcing it to explore and utilize other potentially more relevant features. Across the code and the different datasets and tasks, we always follow the same steps:\n",
        "1. training of a model using empirical risk minimization (ERM);\n",
        "2. the most discriminative features identified by the model are masked using XGradCAM;\n",
        "3. fine-tuning of the model for a single epoch on the modified (masked) dataset.\n",
        "\n",
        "XGradCAM is a technique used to visualize and interpret the decisions of convolutional neural networks. It highlights the regions of an input image that are most influential in the model's prediction by generating a heatmap over the image. This heatmap shows which parts of the image the model focuses on when making a decision, helping to understand how the model interprets visual features.\n",
        "\n",
        "The MaskTune technique encourages the model to discover and rely on a broader set of features, which enhances its robustness and generalization ability.\n",
        "We implemented two key tasks: classification with spurious features and selective classification. For the first one, we used both the MNIST and CelebA datasets, while the second task was performed on the CIFAR10 dataset.\n",
        "\n",
        "*Classification with spurious features*: this task involves training models on datasets where certain irrelevant or misleading features are unintentionally correlated with the target labels, failing to classify correctly new data that doesnâ€™t exhibit the same correlations (for example, associating the grass with the label \"cow\" can cause errors in the classification of images with cows but without grass). In order to show the effectiveness of the MaskTune strategy in this task, a simple CNN was trained on the MNIST dataset to identify digits, introducing colored backgrounds as spurious features, while for CelebA, a pre-trained ResNet-50 model was used to classify the images based on a single attribute. Across these datasets, we applied MaskTune to mask out the spurious features identified by the models, forcing them to rely on more robust and representative features. Further details on the datasets, preprocessing, architectures and results are provided later in the section \"classification with spurious features\".\n",
        "\n",
        "*Selective classification*: this task focuses on the model's ability to abstain from making predictions when uncertain, thereby reducing the risk of incorrect predictions. This task is particularly relevant in scenarios where incorrect decisions can have significant consequences (e.g. medical tasks). We used a ResNet-32 architecture on the CIFAR-10 dataset and the selective classification was performed using a threshold mechanism: while iterating over the validation set during the training phase, the probabilities corresponding to the selected output class were saved and a threshold was computed to drop the lowest 10% of predictions. This threshold corresponds to the highest value among the lowest 10% of the probabilities. The MaskTune approach was shown to improve the models' ability to selectively classify, by making the results more robust to spurious correlations and leading to more reliable and confident predictions. Further details on the dataset, preprocessing, architecture and results are provided later in the section \"selective classification\".\n",
        "\n",
        "Overall, the tasks demonstrated that MaskTune can enhance both classification accuracy and the reliability of selective classification, especially in the presence of spurious features."
      ],
      "metadata": {
        "id": "Lpj54BzaAgvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLATIONS**"
      ],
      "metadata": {
        "id": "zJY1p2-bkmwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n",
        "!pip install timm\n",
        "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git"
      ],
      "metadata": {
        "id": "1uB0ovjqovyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuOieGPhTpo9"
      },
      "source": [
        "**IMPORT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets,transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import timm\n",
        "import random\n",
        "import pickle\n",
        "import shutil\n",
        "from pytorch_grad_cam import XGradCAM\n",
        "import cv2\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torchvision.datasets import CIFAR10\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "WK4xJiL8o9NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #defining the device\n",
        "print(f'Using device: {device}')"
      ],
      "metadata": {
        "id": "9aZ0q_mAo5lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASSES & FUNCTIONS**"
      ],
      "metadata": {
        "id": "KTs_n1q2mnB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions:\n",
        "*   Validate: this function implements the evaluation of the model on the     validation set during the training. The model is set to evaluation mode to compute the loss and the accuracy on the validation data. This is useful to monitor the trend of the loss and avoid possiible overfitting issues.\n",
        "*   save_checkpoints: this function saves the checkpoint in the given checkpoint path.\n",
        "*   load_checkpoint: it is the complementary function of save_checkpoints, it loads the checkpoints in the model to restart from the last learned parameters.\n",
        "*   mask_heatmap_using_threshold: this function has been taken from the paper code. It applies a binary mask to each heatmap according to a threshold computed as the sum of mean value of the heatmap and two times the standard deviation of the heatmap. It returns 0 if the pixel is above the threshold, 1 otherwise.\n",
        "*   mask_tune:this function is used to first generate the heatmaps using XGradCAM heatmap generator, then it generates masks with the previously defined function 'mask_heatmap_using_threshold' and it masks the original images with an element-wise multiplication. It finally saves the masked images according to their label in the specified path.\n",
        "*    bias_transform: this function adds a small 4x4 blue square on the top left corner of the image.\n",
        "*    get_grad_cam_target_layer: this function is taken from the paper code and it is used by the XGradCAM to generate the heatmaps according to the features of the specified layer.\n",
        "*    filter_classes: this function takes only the first 5 classes of the CIFAR10 dataset.\n",
        "*    validate_selective_classification: this function implements a standard evaluation on the validation set (as the previously define 'Validate' function) but it also returns the probability values corresponding to the true class of each sample.\n",
        "\n",
        "Classes:\n",
        "*    CustomDataset: this class is used in order to retrieve previously transformed and preprocessed data which has been saved with torch.save function on google drive. We defined this class in order to avoid repeating the same steps between a colab runtime and another.\n",
        "*    GrayscaleToRGB: this class is used to transform gray scale images to RGB ones. It is needed because, altough MNIST is a grayscale dataset, the addition of the small blue square requires the images to have 3 channels.\n",
        "*   SimpleCNN: this class implements a CNN with 4 convolutional layers (backbone) and 2 final fully connected layers. The backbone consists of a sequence of convolutional layers, batch normalization, ReLU activations, and max pooling.\n",
        "*   LambdaLayer: this class is used for dimensionality reasons in the cases in which the stride is different from 1 or the number of input channels is different from the number of output channels. It implements a downsampling.\n",
        "*   BasicBlock: this class defines the basic block used by the Resnet32 class. It consists of two convolutional layers, two batch normalization layers and two ReLU activation functions (after each convolutional layer) and a downsampling layer (LambdaLayer).\n",
        "*   ResNet32: this class defines a resnet architecture which is constituted by residual blocks (BasicBlock). It first implements a convolutional layer, then it defines three layers, each one composed of 5 basic blocks. The first layer keeps the dimensionality unchanged, while the first block of the other two implements a downsampling. After the last residual block the network does an average pooling and a flattening.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E3jbl784lAMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJoUrLe0cRUE"
      },
      "outputs": [],
      "source": [
        "#this function implements the evaluation of the model on the validation set during the training.\n",
        "#The model is set to evaluation mode to compute the loss and the accuracy on the validation data.\n",
        "#This is useful to monitor the trend of the loss and avoid possiible overfitting issues.\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * data.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = 100. * correct / len(val_loader.dataset)\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "    model.train()\n",
        "    return val_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiJOUM1tTDlk"
      },
      "outputs": [],
      "source": [
        "#this function saves the checkpoint in the given checkpoint path.\n",
        "def save_checkpoint(model, optimizer, epoch, loss, accuracy, checkpoint_path):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKYGCPFaviv3"
      },
      "outputs": [],
      "source": [
        "#it is the complementary function of save_checkpoints,\n",
        "#it loads the checkpoints in the model to restart from the last learned parameters\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "   checkpoint = torch.load(checkpoint_path)\n",
        "   model.load_state_dict(checkpoint['model_state_dict'])\n",
        "   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "   epoch = checkpoint['epoch']\n",
        "   val_loss = checkpoint['loss']\n",
        "   accuracy = checkpoint['accuracy']\n",
        "   return model, optimizer, epoch, val_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGlmVMk_Of5h"
      },
      "outputs": [],
      "source": [
        "#this function has been taken from the paper code.\n",
        "#It applies a binary mask to each heatmap according to a threshold computed as the sum of mean value of the heatmap and two times the standard deviation of the heatmap.\n",
        "#It returns 0 if the pixel is above the threshold, 1 otherwise.\n",
        "def mask_heatmap_using_threshold(heat_maps):\n",
        "    mask_mean_value = np.nanmean(np.where(heat_maps > 0, heat_maps, np.nan), axis=(1, 2))[:, None, None]\n",
        "    mask_std_value = np.nanstd(np.where(heat_maps > 0, heat_maps, np.nan), axis=(1, 2))[:, None, None]\n",
        "    mask_threshold_value = mask_mean_value + 2 * mask_std_value\n",
        "    return np.where(heat_maps > mask_threshold_value, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVAP7qkJCCYh"
      },
      "outputs": [],
      "source": [
        "#this function is used to first generate the heatmaps using XGradCAM heatmap generator,\n",
        "#then it generates masks with the previously defined function 'mask_heatmap_using_threshold' and it masks the original images with an element-wise multiplication.\n",
        "#It finally saves the masked images according to their label in the specified path.\n",
        "def mask_tune(images, masked_data_dir, images_paths, targets):\n",
        "  heat_maps = heat_map_generator(images)\n",
        "  masks = mask_heatmap_using_threshold(heat_maps)\n",
        "  masks = np.repeat(masks[:, np.newaxis, :, :], 3, axis=1)\n",
        "  for image_path, mask, target, data_dir in zip(images_paths, masks, targets, masked_data_dir):\n",
        "    original_image = Image.open(image_path).convert('RGB')\n",
        "    mask=mask.transpose(1, 2, 0)\n",
        "    image_mask = cv2.resize(mask, dsize=original_image.size, interpolation=cv2.INTER_NEAREST)\n",
        "    i=np.array(original_image)*image_mask\n",
        "    target_dir = os.path.join(data_dir, str(target.item()))\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    im = Image.fromarray(i.astype(np.uint8))\n",
        "    im.save(os.path.join(target_dir, image_path.split(\"/\")[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J7EvG17xmlw"
      },
      "outputs": [],
      "source": [
        "#MNIST function\n",
        "#this function adds a small 4x4 blue square on the top left corner of the image.\n",
        "def bias_transform(x):\n",
        "  new_value = torch.tensor([0.0, 0.0, 1.0])\n",
        "  x[:, :4, :4] = new_value.view(3, 1, 1)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10\n",
        "#this function takes only the first 5 classes of the CIFAR10 dataset\n",
        "def filter_classes(dataset, classes=[0, 1, 2, 3, 4]):\n",
        "    indices = [i for i, (_, label) in enumerate(dataset) if label in classes]\n",
        "    subset = torch.utils.data.Subset(dataset, indices)\n",
        "    return subset"
      ],
      "metadata": {
        "id": "1CN-aqNH0tzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10\n",
        "#this function implements a standard evaluation on the validation set\n",
        "#(as the previously define 'Validate' function) but it also returns the probability values corresponding to the true class of each sample.\n",
        "def validate_selective_classification(model, valloader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            selected_probs = probs.gather(1, labels.view(-1, 1)).cpu().numpy()\n",
        "            all_probs.extend(selected_probs)\n",
        "\n",
        "            _, predicted = torch.max(probs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(valloader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    all_probs = np.array(all_probs).flatten()\n",
        "\n",
        "    return val_loss, accuracy, all_probs"
      ],
      "metadata": {
        "id": "-v_rTCPv0Azd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_WI-JlTvCfH"
      },
      "outputs": [],
      "source": [
        "#generated by Chat GPT\n",
        "#this class is used in order to retrieve previously transformed and preprocessed data which has been saved with torch.save function on google drive.\n",
        "#We defined this class in order to avoid repeating the same steps between a colab runtime and another.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data_list = torch.load(file_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data_list[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-fq16CLcedu"
      },
      "outputs": [],
      "source": [
        "#MNIST class generated by Chat GPT\n",
        "#this class is used to transform gray scale images to RGB ones. It is needed because, altough MNIST is a grayscale dataset,\n",
        "#the addition of the small blue square requires the images to have 3 channels.\n",
        "class GrayscaleToRGB:\n",
        "    def __call__(self, img):\n",
        "        return img.repeat(3, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST model with the same hyperparameters of the paper\n",
        "#this class implements a CNN with 4 convolutional layers (backbone) and 2 final fully connected layers.\n",
        "#The backbone consists of a sequence of convolutional layers, batch normalization, ReLU activations, and max pooling.\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.backbone=nn.Sequential( nn.Conv2d(3, 16, (3, 3), (1, 1)),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, (3, 3), (1, 1)),\n",
        "            nn.BatchNorm2d(num_features=16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), (2, 2)),\n",
        "            nn.Conv2d(16, 32, (3, 3), (1, 1)),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, (3, 3), (1, 1)),\n",
        "            nn.BatchNorm2d(num_features=32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), (2, 2)),\n",
        "            nn.Flatten())\n",
        "        self.fc1 = nn.Linear(in_features=512, out_features=256)\n",
        "        self.bnfc=nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self.backbone(x)\n",
        "        x=self.fc1(x)\n",
        "        x = F.relu(self.bnfc(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    #function taken from the paper code\n",
        "    #this function is taken from the paper code and it is used by the XGradCAM to generate the heatmaps according to the features of the specified layer.\n",
        "    def get_grad_cam_target_layer(self):\n",
        "        return self.backbone[-3]"
      ],
      "metadata": {
        "id": "j4NX6sQ7yiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10\n",
        "#this class is used for dimensionality reasons in the cases in which the stride is different from 1 or the number of input channels is different from the number of output channels.\n",
        "#It implements a downsampling.\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)"
      ],
      "metadata": {
        "id": "LVrbqqPWzphM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10\n",
        "#this class defines the basic block used by the Resnet32 class.\n",
        "#It consists of two convolutional layers, two batch normalization layers and two ReLU activation functions (after each convolutional layer) and a downsampling layer (LambdaLayer).\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, out_channels//4, out_channels//4), \"constant\", 0))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.downsample(x)\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "DmTNz4x90JfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CIFAR10 model\n",
        "#this class defines a resnet architecture which is constituted by residual blocks (BasicBlock).\n",
        "#It first implements a convolutional layer, then it defines three layers, each one composed of 5 basic blocks. The first layer keeps the dimensionality unchanged, while the first block of the other two implements a downsampling.\n",
        "#After the last residual block the network does an average pooling and a flattening.\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_channels = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Layer 1\n",
        "        self.layer1_0 = BasicBlock(16, 16, stride=1)\n",
        "        self.layer1_1 = BasicBlock(16, 16, stride=1)\n",
        "        self.layer1_2 = BasicBlock(16, 16, stride=1)\n",
        "        self.layer1_3 = BasicBlock(16, 16, stride=1)\n",
        "        self.layer1_4 = BasicBlock(16, 16, stride=1)\n",
        "\n",
        "        # Layer 2\n",
        "        self.layer2_0 = BasicBlock(16, 32, stride=2)\n",
        "        self.layer2_1 = BasicBlock(32, 32, stride=1)\n",
        "        self.layer2_2 = BasicBlock(32, 32, stride=1)\n",
        "        self.layer2_3 = BasicBlock(32, 32, stride=1)\n",
        "        self.layer2_4 = BasicBlock(32, 32, stride=1)\n",
        "\n",
        "        # Layer 3\n",
        "        self.layer3_0 = BasicBlock(32, 64, stride=2)\n",
        "        self.layer3_1 = BasicBlock(64, 64, stride=1)\n",
        "        self.layer3_2 = BasicBlock(64, 64, stride=1)\n",
        "        self.layer3_3 = BasicBlock(64, 64, stride=1)\n",
        "        self.layer3_4 = BasicBlock(64, 64, stride=1)\n",
        "\n",
        "\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Layer 1\n",
        "        x = self.layer1_0(x)\n",
        "        x = self.layer1_1(x)\n",
        "        x = self.layer1_2(x)\n",
        "        x = self.layer1_3(x)\n",
        "        x = self.layer1_4(x)\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.layer2_0(x)\n",
        "        x = self.layer2_1(x)\n",
        "        x = self.layer2_2(x)\n",
        "        x = self.layer2_3(x)\n",
        "        x = self.layer2_4(x)\n",
        "\n",
        "        # Layer 3\n",
        "        x = self.layer3_0(x)\n",
        "        x = self.layer3_1(x)\n",
        "        x = self.layer3_2(x)\n",
        "        x = self.layer3_3(x)\n",
        "        x = self.layer3_4(x)\n",
        "\n",
        "        x = F.avg_pool2d(x, x.size()[3])\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def get_grad_cam_target_layer(self):\n",
        "        return self.layer3_4"
      ],
      "metadata": {
        "id": "L35fnejJ0Sn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl2M6iQAy5Fh"
      },
      "source": [
        "**CLASSIFICATION WITH SPURIOUS FEATURES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtRl-C8dTmtj"
      },
      "source": [
        "MNIST\n",
        "\n",
        "*   Dataset: The MNIST dataset contains 60000 training images (we split them into 48000 for training and 12000 for validation) and 10000 testing images. The dataset is composed by low resolution (28x28) greyscale images of white numbers (from 0 to 9) on a black background and this is often used for simple classification tasks.\n",
        "*   Preprocessing: In order to show the ability of MaskTune to not rely on spurious features, we started with a basic example. We preprocessed our data firstly by creating two subgroups from the original 10 classes (classes from 0 to 4 are put in the new class 0, classes from 5 to 9 in the new class 1) and secondly by adding a small 4x4 blue square to some of the images (spurious feature). Moreover, in order to add the blue square, we performed a grayscale to RGB transformation on all the data and Image to Tensor transformations are also implemented across the code.\n",
        "*   Architecture: The SimpleCNN model is a convolutional neural network designed to classify images into two classes. The network consists of four convolutional layers. The first two convolutional layers have 16 filters each, with a kernel size of 3x3 and a stride of 1. These layers are followed by batch normalization to stabilize and speed up training, and the ReLU activation function is applied to introduce non-linearity. After the second convolutional layer, a max-pooling operation with a 2x2 filter and a stride of 2 is used to downsample the feature maps, reducing their spatial dimensions. The third and fourth convolutional layers have 32 filters each, again with a 3x3 kernel size and a stride of 1, followed by batch normalization and ReLU activation. After the fourth convolutional layer, another 2x2 max-pooling operation with a stride of 2 is applied. The output from the final max-pooling layer is flattened into a 1D vector with 512 elements. This flattened vector is then passed through a fully connected layer with 256 units, followed by batch normalization and the ReLU activation function. Finally, the network includes a second fully connected layer that outputs logits corresponding to the two classes.\n",
        "For training, the model uses the CrossEntropyLoss function, which is standard for classification tasks. The optimization is performed using the Stochastic Gradient Descent (SGD) algorithm with a learning rate of 0.01, a momentum of 0.9, and a weight decay of 1e-4 to help prevent overfitting. To further refine the learning process,the MultiStepLR scheduler is employed to decrease the learning rate by a factor of 0.5 at specific epochs (specified by the parameter milestones). The model is trained for a total of 100 epochs with a batch size of 128 for both the training and validation datasets. Throughout the training process, the model's performance on the validation set is monitored, and the best model, based on validation accuracy, is saved. At the end of the 100 epochs, a final checkpoint of the model is also saved.\n",
        "*   Results: the SimpleCNN model is first evaluated using the best checkpoint on both the original and biased test sets. The ERM model achieves an accuracy of 0.9352 on the original test set, while its performance on the biased test set is slightly lower, with an accuracy of 0.8977. This discrepancy highlights the model's sensitivity to bias in the data. However, after applying the MaskTune technique, the model's performance improves significantly on both test sets. The accuracy on the original one increases to 0.9550, and the accuracy on the biased test set also rises to 0.9540. This result demonstrates that MaskTune effectively reduces the accuracy gap between the original and biased test sets, indicating enhanced robustness and fairness in the model's predictions compared to the initial results using ERM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLKKVUR6a3Of"
      },
      "outputs": [],
      "source": [
        "#It performs a composition of grayscale to RGB and Image to Tensor transformations to the MNIST samples.\n",
        "transform = transforms.Compose([transforms.ToTensor(), GrayscaleToRGB()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading MNIST dataset from torchvision\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "YLeVRaEIFrZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGBrf5vyK-v8"
      },
      "outputs": [],
      "source": [
        "#dividing the 10 MNIST training set classes in 2 groups [0: 0-4 classes; 1: 5-9 classes]\n",
        "class_0 = []\n",
        "class_1 = []\n",
        "for i in mnist_trainset:\n",
        "  if(i[1] < 5):\n",
        "    class_0.append(i)\n",
        "  else:\n",
        "    class_1.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er-JhL6FJ6Wz"
      },
      "outputs": [],
      "source": [
        "#create the directory for saving the MNIST checkpoints\n",
        "checkpoints_mnist = '/content/drive/My Drive/checkpoints_mnist/'\n",
        "os.makedirs(checkpoints_mnist, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ9_8RF-x83o"
      },
      "outputs": [],
      "source": [
        "#small blue squares added to 99% of class 0\n",
        "class0 = []\n",
        "num_unchanged=len(class_0)//100\n",
        "unchanged_indices = random.sample(range(len(class_0)), num_unchanged)\n",
        "for i in range(len(class_0)):\n",
        "  if(i in unchanged_indices):\n",
        "    class0.append((class_0[i][0], 0))\n",
        "  else:\n",
        "    class0.append((bias_transform(class_0[i][0]),0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKoTd8MQzWeE"
      },
      "outputs": [],
      "source": [
        "#small blue squares added to 1% of class 1\n",
        "class1 = []\n",
        "num_changed=len(class_1)//100\n",
        "changed_indices = random.sample(range(len(class_1)), num_changed)\n",
        "for i in range(len(class_1)):\n",
        "  if(i in changed_indices):\n",
        "    class1.append((bias_transform(class_1[i][0]), 1))\n",
        "  else:\n",
        "    class1.append((class_1[i][0],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d5zL-BdbOIJ"
      },
      "outputs": [],
      "source": [
        "#creation of the full dataset\n",
        "total_trainset = class0 + class1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb9fMKxkXzsa"
      },
      "outputs": [],
      "source": [
        "#we sampled 12000 indices randomly from the total trainset and saved them on google drive for repeatability\n",
        "val_set_indices=random.sample(range(len(total_trainset)),k=12000)\n",
        "file_path = 'val_set_indices.pkl'\n",
        "with open(file_path, 'wb') as f:\n",
        "    pickle.dump(val_set_indices, f)\n",
        "drive_path = '/content/drive/My Drive/val_set_indices.pkl'\n",
        "shutil.copy(file_path, drive_path)\n",
        "os.remove(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B70-Uy5_ljRV"
      },
      "outputs": [],
      "source": [
        "#we define the val and train sets according to the previous random sampling\n",
        "drive_path = '/content/drive/My Drive/val_set_indices.pkl'\n",
        "with open(drive_path, 'rb') as f:\n",
        "    val_set_indices = pickle.load(f)\n",
        "val_set = [total_trainset[i] for i in val_set_indices]\n",
        "train_set = [total_trainset[i] for i in range(len(total_trainset)) if i not in val_set_indices]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we saved on google drive the MNIST trainset\n",
        "mnist_dataset = '/content/drive/My Drive/MNISTdatatrainset/'\n",
        "os.makedirs(mnist_dataset, exist_ok=True)\n",
        "i=0\n",
        "for el in train_set:\n",
        "  image=el[0]\n",
        "  t=transforms.ToPILImage()\n",
        "  t(image).save(mnist_dataset+str(i)+'.jpg')\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "QsIQ8UfeFWm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is0gMhmBZ4Jv"
      },
      "outputs": [],
      "source": [
        "#creation of a list containing the elements of the drive folder.\n",
        "dir_mnist=os.listdir('/content/drive/My Drive/MNISTdatatrainset/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3SNCWgVDnAk"
      },
      "outputs": [],
      "source": [
        "#dividing the 10 MNIST original testing set classes in 2 groups [0: 0-4 classes; 1: 5-9 classes]\n",
        "original_testset= []\n",
        "for i in mnist_testset:\n",
        "  if(i[1]<5):\n",
        "   original_testset.append((i[0],0))\n",
        "  else:\n",
        "    original_testset.append((i[0],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD3wN5qbeZj2"
      },
      "outputs": [],
      "source": [
        "#we added a small blue square to the test images of classes 5-9\n",
        "biased_testset_initial = []\n",
        "for i in mnist_testset:\n",
        "  x,y = i\n",
        "  if(i[1]>4):\n",
        "    biased_testset_initial.append((bias_transform(x),y))\n",
        "  else:\n",
        "    biased_testset_initial.append((x,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RrfZ9kBE2gn"
      },
      "outputs": [],
      "source": [
        "#dividing the 10 MNIST biased testing set classes in 2 groups [0: 0-4 classes; 1: 5-9 classes]\n",
        "biased_testset = []\n",
        "for i in biased_testset_initial:\n",
        "  if i[1]<5:\n",
        "    biased_testset.append((i[0],0))\n",
        "  else:\n",
        "    biased_testset.append((i[0],1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHRy_JnraGXG"
      },
      "outputs": [],
      "source": [
        "#SimpleCNN training with MNIST dataset\n",
        "train_loader_MNIST = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=8) #train data loader\n",
        "val_loader_MNIST = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=8) #val data loader\n",
        "\n",
        "model = SimpleCNN(num_classes=2).to(device) #Model used for the training moved to Device\n",
        "loss = nn.CrossEntropyLoss() #cross entropy loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum = 0.9, weight_decay = 1e-4) #SGD optimizer\n",
        "lr_step = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50, 75, 100], gamma=0.5) #MultiStepLR learning rate scheduler\n",
        "best_accuracy = -math.inf #initialization of the best accuracy\n",
        "\n",
        "num_epochs = 100 #number of epochs\n",
        "epoch = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    batch_count=0\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader_MNIST):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        batch_count +=1\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss_f = loss(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss_f.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_f.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader_MNIST.dataset)\n",
        "    val_loss, val_accuracy = validate(model, val_loader_MNIST, loss)\n",
        "    lr_step.step()\n",
        "    if val_accuracy > best_accuracy: #compute the best checkpoint\n",
        "        best_accuracy = val_accuracy\n",
        "        best_checkpoint_path = f'/content/drive/My Drive/checkpoints_mnist/best_checkpoint.pth'\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, val_accuracy, best_checkpoint_path) #save the best checkpoint\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    if epoch == num_epochs - 1:\n",
        "        last_checkpoint_path = f'/content/drive/My Drive/checkpoints_mnist/last_checkpoint.pth'\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, val_accuracy, last_checkpoint_path) # save the last checkpoint\n",
        "print('Training finished.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#definition of the two data loaders for the two test sets\n",
        "test_loader_MNIST = DataLoader(original_testset, batch_size=128, shuffle=False, num_workers=8)\n",
        "biased_test_loader_MNIST = DataLoader(biased_testset, batch_size=128, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "rxMehALFFvLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi1aTEuSPRZr"
      },
      "outputs": [],
      "source": [
        "#definition of the Model\n",
        "model = SimpleCNN(num_classes=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgDFe2pPOHyy"
      },
      "outputs": [],
      "source": [
        "#paths to the best and last checkpoints\n",
        "best_checkpoint_mnist_path='/content/drive/My Drive/checkpoints_mnist/best_checkpoint.pth'\n",
        "last_checkpoint_mnist_path='/content/drive/My Drive/checkpoints_mnist/last_checkpoint.pth'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing ERM model (SImpleCNN) for original test set.\n",
        "model.load_state_dict(torch.load(best_checkpoint_mnist_path)['model_state_dict'])#loading the model parameters saved in the best checkpoint dict\n",
        "model.eval() #setting the model to evaluation mode\n",
        "model.to(device) #moving the model to device\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): #disabling gradient calculation\n",
        "    for inputs, labels in test_loader_MNIST:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #inputs and labels moved to device\n",
        "        outputs = model(inputs) #applying model to inputs\n",
        "        _, predicted = torch.max(outputs, 1) #predictions\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions) #computing accuracy\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "9srQIFhypeIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing ERM model for biased test set\n",
        "model.load_state_dict(torch.load(best_checkpoint_mnist_path)['model_state_dict']) #loading the model parameters saved in the best checkpoint dict\n",
        "model.to(device)\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in biased_test_loader_MNIST: #now we use the biased test set\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "C8BT8yt3pglc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the model parameters saved in the best checkpoint dict\n",
        "model.load_state_dict(torch.load(best_checkpoint_mnist_path)['model_state_dict'])"
      ],
      "metadata": {
        "id": "eVpFUw9xFzyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qwoym9SZY1s"
      },
      "outputs": [],
      "source": [
        "#heat map generator using XGradCAM taken from the github link installed previously\n",
        "heat_map_generator = XGradCAM(model=model, target_layers=[model.get_grad_cam_target_layer()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO5Xq2XtWgcN"
      },
      "outputs": [],
      "source": [
        "#directory containing masked data\n",
        "masked_data_dir_mnist = '/content/drive/My Drive/maskeddataMNIST/'\n",
        "os.makedirs(masked_data_dir_mnist, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiENbZLuorPt"
      },
      "outputs": [],
      "source": [
        "#we save in a list the training tensors, the directory where to save the corresponding masked data, the image path and the label.\n",
        "mnist_list=[]\n",
        "for i in range(len(dir_mnist)):\n",
        "  mnist_list.append((train_set[i][0],masked_data_dir_mnist,mnist_dataset+str(i)+'.jpg', train_set[i][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9PhKvqFgCkh"
      },
      "outputs": [],
      "source": [
        "#directory to save the data to mask\n",
        "trainset_mnist_to_mask = '/content/drive/My Drive/trainset_mnist_to_mask/'\n",
        "os.makedirs(trainset_mnist_to_mask, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we saved the MNIST training set with all the information needed for mask tune [tensor, directory of masked data, original_img_path, label]\n",
        "torch.save(mnist_list, trainset_mnist_to_mask+'tensors_list.pth')"
      ],
      "metadata": {
        "id": "gN2VLZWyFaXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definition of the loader that is used in the testing of MaskTune\n",
        "mask_loader_MNIST = torch.utils.data.DataLoader(CustomDataset(trainset_mnist_to_mask+'tensors_list.pth'), batch_size=128, shuffle=False, num_workers=8)"
      ],
      "metadata": {
        "id": "BwMlja3xFdTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we perform the function MaskTune for all the data in the loader.\n",
        "for data in mask_loader_MNIST:\n",
        "  images, save_dir, images_pathes, targets = data[0], data[1], data[2], data[3]\n",
        "  images = images.to(device) #move the tensors to device\n",
        "  mask_tune(images, save_dir, images_pathes, targets)"
      ],
      "metadata": {
        "id": "FfDvUVRIvDxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhtv5oG2AXKZ"
      },
      "outputs": [],
      "source": [
        "dir0 ='/content/drive/My Drive/maskeddataMNIST/0/'\n",
        "dir1 ='/content/drive/My Drive/maskeddataMNIST/1/'\n",
        "dir0list = os.listdir(dir0)\n",
        "dir1list = os.listdir(dir1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srn5ocpHC3V6"
      },
      "outputs": [],
      "source": [
        "#defining the image to tensor transform\n",
        "transf=transforms.Compose([transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5eKFkJIBvx6"
      },
      "outputs": [],
      "source": [
        "#for each element in class 0 of the masked data we apply the transformation from image to tensor\n",
        "mask_data_mnist_transformed0 = []\n",
        "target = 0\n",
        "\n",
        "for el in dir0list:\n",
        "  img = Image.open(dir0+el)\n",
        "  img = transf(img)\n",
        "  mask_data_mnist_transformed0.append((img, target)) #list with transformed images and target 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP1HZrSyCOmg"
      },
      "outputs": [],
      "source": [
        "#for each element in class 1 of the masked data we apply the transformation from image to tensor\n",
        "mask_data_mnist_transformed1 = []\n",
        "target = 1\n",
        "\n",
        "for el in dir1list:\n",
        "  img = Image.open(dir1+el)\n",
        "  img = transf(img)\n",
        "  mask_data_mnist_transformed1.append((img, target)) #list with transformed images and target 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_pomcX1DOyh"
      },
      "outputs": [],
      "source": [
        "#union of the two lists\n",
        "mask_data_mnist_transformed=mask_data_mnist_transformed0+mask_data_mnist_transformed1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mjwXmP0Ndo1"
      },
      "outputs": [],
      "source": [
        "masktuned_mnist='/content/drive/My Drive/masktuned_mnist/'\n",
        "os.makedirs(masktuned_mnist, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmpvkehYDvij"
      },
      "outputs": [],
      "source": [
        "torch.save(mask_data_mnist_transformed, '/content/drive/My Drive/masktuned_mnist/mask_data_mnist_transformed.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_mnist_mask_path = '/content/drive/My Drive/masktuned_mnist/mask_data_mnist_transformed.pth'\n",
        "masked_loader_MNIST = torch.utils.data.DataLoader(CustomDataset(transformed_mnist_mask_path), batch_size=128, shuffle=True, num_workers=8)"
      ],
      "metadata": {
        "id": "elX5acn6F2qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkZDb4JlLPmP",
        "outputId": "df97078f-3e5f-4dc3-ffc5-a419f740ace8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 0.0362\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "#training the SimpleCNN for one epoch on the new masked training dataset with the same hyperparameters specified in the paper\n",
        "loss = nn.CrossEntropyLoss() #definition of the loss\n",
        "model = SimpleCNN(num_classes=2).to(device) #moving the model to device\n",
        "model.load_state_dict(torch.load(last_checkpoint_mnist_path)['model_state_dict']) #loading the model parameters saved in the last checkpoint\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum = 0.9, weight_decay = 1e-4) #SGD optimizer\n",
        "num_epochs = 1\n",
        "\n",
        "model.train() #setting the model to training mode\n",
        "batch_count = 0\n",
        "running_loss = 0.0\n",
        "for batch_idx, (inputs, labels) in enumerate(masked_loader_MNIST):\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        batch_count +=1\n",
        "        optimizer.zero_grad() #Reset the gradients of all model parameters to zero before backpropagation\n",
        "\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss_f = loss(outputs, labels)\n",
        "        loss_f.backward() #backpropagation\n",
        "        optimizer.step() #update the model parameters\n",
        "\n",
        "        running_loss += loss_f.item() * inputs.size(0) #adding the current loss\n",
        "\n",
        "epoch_loss = running_loss / len(masked_loader_MNIST.dataset) #computing the average loss per epoch\n",
        "print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "if epoch == num_epochs - 1:\n",
        "        last_checkpoint_path = f'/content/drive/My Drive/checkpoints_mnist/last_checkpoint_mask_mnist.pth'\n",
        "        torch.save(model.state_dict(), last_checkpoint_path) #saving the last chekpoint\n",
        "print('Training finished.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t_xyUmez-Y1"
      },
      "outputs": [],
      "source": [
        "#path to the last mnist checkpoint\n",
        "checkpoint_masktune_mnist='/content/drive/My Drive/checkpoints_mnist/last_checkpoint_mask_mnist.pth'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing of Mask tune model for original test set\n",
        "model.load_state_dict(torch.load(checkpoint_masktune_mnist)) #loading the model parameters saved in the last checkpoint\n",
        "model.eval() #setting the model to evaluation mode\n",
        "model.to(device) #moving the model to device\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): #disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader_MNIST:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving inputs and labels to device\n",
        "        outputs = model(inputs) #applying the model to the inputs\n",
        "        _, predicted = torch.max(outputs, 1) #predictions\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions)#computing the accuracy\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "U7bwz8QEpkES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing of Mask tune model for biased test set\n",
        "model.load_state_dict(torch.load(checkpoint_masktune_mnist))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in biased_test_loader_MNIST:#now we use the biased test data loader\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "HnMyuYIypmSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZjYeC3W1pg2"
      },
      "source": [
        "CELEBA\n",
        "\n",
        "*   Dataset: The CelebA dataset is a large-scale face dataset which consists of over 200,000 celebrity images (178x218), each annotated with 40 binary attribute labels that describe various facial features and characteristics., such as gender, hair color, eyeglasses, smile, and more.  The dataset is divided into three subsets: training, validation, and testing.\n",
        "*   Preprocessing: Because of RAM issues, we split the training set (162770 samples) into 17 smaller groups of 10000 samples each, and we trained our model on one of these subgroups. To keep the proportions, we used 1250 samples in the validation set and 1250 samples for test set. For our task, we dropped all the attribute labels except the hair color. This was done to reduce our problem to a binary classification task, as the authors did. In the dataset, the column 'Blond_Hair' contains the value -1 when the feature is not present and 1 otherwise, so we substituted all -1 with the value 0, in order to implement the classification task. Moreover, for the training set images are randomly resized and cropped to the target resolution (224x224), with a scale factor between 0.7 and 1.0 and an aspect ratio between 1.0 and 1.33. Additionally, the images are randomly flipped horizontally to augment the dataset. After these augmentations, the images are converted to tensors and normalized using mean and standard deviation values. For the testing and validation sets, images are first center-cropped to the minimum dimension, then resized to the target resolution. Like in the training set, they are converted to tensors and normalized. These transformations ensure that the images are prepared consistently for model input while also introducing variability in the training phase to improve generalization.\n",
        "*   Architecture: We use a pre-trained ResNet-50 model and we fine-tune it for our specific task by adjusting it to have two output classes. For training, the model is optimized using stochastic gradient descent (SGD) with a learning rate of 1e-4, momentum of 0.9, and weight decay of 1e-4 to help prevent overfitting. The training loop is run for 5 epochs. During each epoch, the model is put into training mode, and it iterates over the training dataset in batches. The outputs of the model are then compared with the true labels using the CrossEntropyLoss function, which computes the loss. The loss is backpropagated, scaled appropriately using the GradScaler to prevent issues related to small gradient values, and the model's weights are updated using the optimizer. After each epoch, the average training loss is calculated, and the model's performance is evaluated on a validation set. If the accuracy on the validation set improves over previous epochs, the model is saved as the best checkpoint. At the end of the training process the final model checkpoint is saved as well.\n",
        "*   Results: the ERM model achieves on the test set an accuracy of 0.88, but its F1 score is low (0.0506), indicating poor balance between precision and recall. The precision is 1.0000, suggesting the model makes very few false positives, but the recall is extremely low at 0.0260, indicating that the model misses many true positives. The ROC AUC score is 0.5129 and the confusion matrix shows that the model correctly classifies 1096 negatives but only 4 positives, with 150 positives misclassified as negatives. In contrast, the MaskTune model shows improved overall performance. The accuracy increases to 0.8960, and the F1 score improves significantly to 0.3011, reflecting a better trade-off between precision and recall. Precision drops slightly to 0.8750, but recall improves to 0.1818, indicating that the model is better at identifying true positives compared to the ERM model. The ROC AUC score also increases to 0.5891, suggesting better discrimination between classes. The confusion matrix for MaskTune shows that the model correctly classified 1092 negatives and 28 positives, with 126 positives incorrectly classified as negatives. Overall, MaskTune demonstrates better balanced performance, particularly in terms of recall and F1 score, compared to the ERM model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We downloaded on google drive the CELEBA dataset from the link provided by the paper authors\n",
        "!unzip \"/content/drive/My Drive/archive.zip\" -d \"/content/data/\""
      ],
      "metadata": {
        "id": "t3UrdveTF6JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reading the csv containing the unzipped dataset and take only the selected columns\n",
        "dataset_celeba = pd.read_csv('/content/data/list_attr_celeba.csv', usecols = ['image_id','Blond_Hair'])\n",
        "split = pd.read_csv('/content/data/list_eval_partition.csv')"
      ],
      "metadata": {
        "id": "XJNqxuI7y0lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given the fact that the training data in the dataset are the first 162770, we dropped all the validation and test data.\n",
        "#Then, we used Chat GPT in order to divide randomly the 162770 into 17 smaller subsets (10000 samples each except the last one) because of time and RAM issues.\n",
        "#The rest of the code refers to the first subset of training data (dataset_celeba0)\n",
        "dropfromindx= 162770\n",
        "dataset_celeba_train=dataset_celeba.drop(dataset_celeba.index[dropfromindx:])\n",
        "num_samples = dataset_celeba_train.shape[0] // 10000\n",
        "sample_size = 10000\n",
        "\n",
        "for i in range(num_samples):\n",
        "    if len(dataset_celeba_train) < sample_size:\n",
        "        break\n",
        "    sampled_df = dataset_celeba_train.sample(n=sample_size, random_state=42+i)\n",
        "    dataset_celeba_train = dataset_celeba_train.drop(sampled_df.index)\n",
        "\n",
        "    sampled_df.to_csv(f'/content/drive/My Drive/sampled_data_celeba/sampled_data_{i}.csv')\n",
        "\n",
        "if not dataset_celeba_train.empty:\n",
        "    dataset_celeba_train.to_csv(f'/content/drive/My Drive/sampled_data_celeba/sampled_data_{num_samples}.csv')\n",
        "\n",
        "dataset_celeba0 = pd.read_csv('/content/drive/My Drive/sampled_data_celeba/sampled_data_0.csv')"
      ],
      "metadata": {
        "id": "dF0q8YB00icP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKhCiZ1n49Wd"
      },
      "outputs": [],
      "source": [
        "#the csv containing the original dataset has a 'partition' column which splits the data into train, validation and test according to 0, 1, 2.\n",
        "#Moreover, the column 'Blond_Hair' contains the value -1 when the feature is not present. We substituted it with the value 0\n",
        "valset_celeba = []\n",
        "testset_celeba = []\n",
        "slideindex=162770\n",
        "\n",
        "\n",
        "for i in dataset_celeba.values[slideindex:]:\n",
        "  ind=dataset_celeba.index[slideindex]\n",
        "  splitcl=split['partition'][ind]\n",
        "  if (splitcl==1):\n",
        "    if i[1]==1:\n",
        "     valset_celeba.append([i[0],i[1]])\n",
        "    else:\n",
        "      valset_celeba.append([i[0],0])\n",
        "  elif (splitcl==2):\n",
        "    if i[1]==1:\n",
        "     testset_celeba.append([i[0],i[1]])\n",
        "    else:\n",
        "      testset_celeba.append([i[0],0])\n",
        "  slideindex +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM2PrxflzKsz"
      },
      "outputs": [],
      "source": [
        "#we did the same substitution [-1 to 0] for the training set\n",
        "trainset_celeba = []\n",
        "\n",
        "for i in dataset_celeba0.values:\n",
        "  ind=i[0]\n",
        "  splitcl=split['partition'][ind]\n",
        "  if(splitcl==0):\n",
        "    if i[2]==1:\n",
        "     trainset_celeba.append([i[1],i[2]])\n",
        "    else:\n",
        "      trainset_celeba.append([i[1],0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwzZHD_pdf6Q"
      },
      "outputs": [],
      "source": [
        "#paths to folders where we save respectively the transformed training set, masked set, validation set and test set\n",
        "dataset_celeba0_transformed_train = '/content/drive/My Drive/dataset_celeba0_transformed_train/'\n",
        "os.makedirs(dataset_celeba0_transformed_train, exist_ok=True)\n",
        "dataset_celeba0_5epochs_transformed_mask='/content/drive/My Drive/dataset_celeba0_5epochs_transformed_mask/'\n",
        "os.makedirs(dataset_celeba0_5epochs_transformed_mask, exist_ok=True)\n",
        "valset_celeba_transformed = '/content/drive/My Drive/valset_celeba_transformed/'\n",
        "os.makedirs(valset_celeba_transformed, exist_ok=True)\n",
        "testset_celeba_transformed = '/content/drive/My Drive/testset_celeba_transformed/'\n",
        "os.makedirs(testset_celeba_transformed, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_h1WEP33ldx"
      },
      "outputs": [],
      "source": [
        "#we used the same transformations as the authors\n",
        "orig_min_dim = (178,218) #original image resolution\n",
        "target_resolution = (224, 224) #target resolution\n",
        "\n",
        "\n",
        "t_train = transforms.Compose([ #data augmentation\n",
        "            transforms.RandomResizedCrop(\n",
        "                target_resolution,\n",
        "                scale=(0.7, 1.0),\n",
        "                ratio=(1.0, 1.3333333333333333),\n",
        "                interpolation=2),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(), #image to tensor\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #normalization\n",
        "        ])\n",
        "\n",
        "t_test = transforms.Compose([\n",
        "                transforms.CenterCrop(orig_min_dim),\n",
        "                transforms.Resize(target_resolution),\n",
        "                transforms.ToTensor(), #image to tensor\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC87bC47Wz_F"
      },
      "outputs": [],
      "source": [
        "#applying transformations to train set\n",
        "dir = '/content/data/img_align_celeba/img_align_celeba/'\n",
        "train = []\n",
        "for el in trainset_celeba:\n",
        "  img = Image.open(dir+el[0])\n",
        "  img = t_train(img)\n",
        "  train.append((img, el[1]))\n",
        "\n",
        "#saving the transformed tensors\n",
        "torch.save(train, '/content/drive/My Drive/dataset_celeba0_transformed_train/tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aODyzKbgAlyZ"
      },
      "outputs": [],
      "source": [
        "#we sampled randomly 1250 samples from the total validation set and saved them in a csv file on google drive (Chat GPT)\n",
        "valset_celeba_restr=random.sample(valset_celeba, k=1250)\n",
        "\n",
        "image_ids_to_keep = set(item[0] for item in valset_celeba_restr) # Create a set of image IDs from the 'valset_celeba_restr' variable\n",
        "filtered_df = split[split['image_id'].isin(image_ids_to_keep)] # Filter the 'split' DataFrame to include only rows where the 'image_id' is in the set of image IDs\n",
        "output_path_filtered = '/content/drive/MyDrive/valset_celeba_restr.csv' # Define the output path where the filtered DataFrame will be saved as a CSV file\n",
        "filtered_df.to_csv(output_path_filtered, index=False) # Save the filtered DataFrame to the specified output path without including the DataFrame index in the CSV file\n",
        "\n",
        "filtered_val_df = pd.read_csv('/content/drive/MyDrive/valset_celeba_restr.csv')\n",
        "valset_celeba_restr=[]\n",
        "for i in filtered_val_df.values:\n",
        "  for j in valset_celeba:\n",
        "   if i[0]== j[0]: # Check if the 'image_id' in the filtered DataFrame matches the 'image_id' in the original DataFrame\n",
        "    valset_celeba_restr.append(j) # Append the corresponding row from the original DataFrame to the new list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x28yWMJ7-b21"
      },
      "outputs": [],
      "source": [
        "#applying transformations to validation set\n",
        "dir = '/content/data/img_align_celeba/img_align_celeba/'\n",
        "val = []\n",
        "for el in valset_celeba_restr:\n",
        "  img = Image.open(dir+el[0])\n",
        "  img = t_test(img)\n",
        "  val.append((img, el[1]))\n",
        "\n",
        "#saving the transformed tensors\n",
        "torch.save(val, '/content/drive/My Drive/valset_celeba_transformed/tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE_Mjdgnii69"
      },
      "outputs": [],
      "source": [
        "#we sampled randomly 1250 samples from the total test set and saved them in a csv file on google drive (Chat GPT)\n",
        "testset_celeba_restr=random.sample(testset_celeba, k=1250)\n",
        "\n",
        "image_ids_to_keep = set(item[0] for item in testset_celeba_restr)# Create a set of image IDs from the 'testset_celeba_restr' variable\n",
        "filtered_df = split[split['image_id'].isin(image_ids_to_keep)]  # Filter the 'split' DataFrame to include only rows where the 'image_id' is in the set of image IDs\n",
        "output_path_filtered = '/content/drive/MyDrive/testset_celeba_restr.csv' # Define the output path where the filtered DataFrame will be saved as a CSV file\n",
        "filtered_df.to_csv(output_path_filtered, index=False) # Save the filtered DataFrame to the specified output path without including the DataFrame index in the CSV file\n",
        "\n",
        "filtered_test_df = pd.read_csv('/content/drive/MyDrive/testset_celeba_restr.csv')\n",
        "testset_celeba_restr=[]\n",
        "\n",
        "for i in filtered_test_df.values:\n",
        "  for j in testset_celeba:\n",
        "   if i[0]== j[0]:# Check if the 'image_id' in the filtered DataFrame matches the 'image_id' in the original DataFrame\n",
        "    testset_celeba_restr.append(j) # Append the corresponding row from the original DataFrame to the new list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_fgZmgQ3505"
      },
      "outputs": [],
      "source": [
        "#applying transformations to test set\n",
        "dir = '/content/data/img_align_celeba/img_align_celeba/'\n",
        "test = []\n",
        "for el in testset_celeba_restr:\n",
        "  img = Image.open(dir+el[0])\n",
        "  img = t_test(img)\n",
        "  test.append((img, el[1]))\n",
        "\n",
        "#saving the transformed tensors\n",
        "torch.save(test, '/content/drive/My Drive/testset_celeba_transformed/tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we created a pretrained resnet50 model (as the authors did) and moved it to device\n",
        "resnet50 = timm.create_model('resnet50', pretrained = True, num_classes = 2)\n",
        "resnet50 = resnet50.to(device)"
      ],
      "metadata": {
        "id": "tRXzPx3ZSq68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUrrTSy01PCO"
      },
      "outputs": [],
      "source": [
        "#paths to transformed train, validation and test tensors\n",
        "transformed_data0_path = '/content/drive/My Drive/dataset_celeba0_transformed_train/tensors_list.pth'\n",
        "train_loader = torch.utils.data.DataLoader(CustomDataset(transformed_data0_path), batch_size=8, shuffle=True, num_workers=2)\n",
        "transformed_valset_path = '/content/drive/My Drive/valset_celeba_transformed/tensors_list.pth'\n",
        "val_loader = torch.utils.data.DataLoader(CustomDataset(transformed_valset_path), batch_size=8, shuffle=False, num_workers=2)\n",
        "transformed_testset_path = '/content/drive/My Drive/testset_celeba_transformed/tensors_list.pth'\n",
        "test_loader = torch.utils.data.DataLoader(CustomDataset(transformed_testset_path), batch_size=8, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M62tR5T_-xK"
      },
      "outputs": [],
      "source": [
        "#creation of the train, validation and test data loaders with batch size 8 and shuffle parameter true for the train loader and false for the other two\n",
        "train_loader = DataLoader(train, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val, batch_size=8, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test, batch_size=8, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxQgZQHaBw42"
      },
      "outputs": [],
      "source": [
        "#directory to checkpoints\n",
        "checkpoint_dir = '/content/drive/My Drive/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9S1TXdKWtv"
      },
      "outputs": [],
      "source": [
        "#paths to the best and last checkpoints\n",
        "checkpoint_path_last = '/content/drive/MyDrive/checkpoints/last_checkpoint0_5epochs.pth'\n",
        "checkpoint_path_best = '/content/drive/MyDrive/checkpoints/best_checkpoint0_5epochs.pth'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Because of time and RAM issues on Colab which forced us to use only 10000 training samples, we trained the model only for 5 epochs instead of 20\n",
        "scaler = GradScaler() # Initialize the gradient scaler (helps with faster computations)\n",
        "loss = nn.CrossEntropyLoss() #loss function\n",
        "optimizer = optim.SGD(resnet50.parameters(), lr=1e-4, momentum = 0.9, weight_decay = 1e-4) #SGD optimizer\n",
        "\n",
        "num_epochs = 5\n",
        "best_accuracy = 0.0 #initialize the best accuracy tracker\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        " resnet50.train() # Set the model to training mode\n",
        " batch_count = 0\n",
        " running_loss = 0.0\n",
        " for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move the inputs and labels to device\n",
        "        batch_count += 1\n",
        "        optimizer.zero_grad() #Reset the gradients of all model parameters to zero before backpropagation\n",
        "\n",
        "        with autocast(enabled=torch.cuda.is_available()): # Use automatic mixed precision if CUDA is available for faster computations and memory savings\n",
        "         outputs = resnet50(inputs) # Forward pass: compute model predictions\n",
        "         loss_f = loss(outputs, labels) # Compute the loss using the predictions and true labels\n",
        "        scaler.scale(loss_f).backward() # Backward pass: scale the loss and compute gradients\n",
        "        scaler.step(optimizer) # optimizer step\n",
        "        scaler.update()\n",
        "        running_loss += loss_f.item() * inputs.size(0)\n",
        "\n",
        " epoch_loss = running_loss / len(train_loader.dataset) # Calculate the average loss for the epoch\n",
        " val_loss, val_accuracy = validate(resnet50, val_loader, loss) # Validate the model on the validation set and obtain validation loss and accuracy\n",
        "\n",
        "# Save the model checkpoint if the validation accuracy improves\n",
        " if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_checkpoint_path = f'/content/drive/My Drive/checkpoints/best_checkpoint0_5epochs.pth'\n",
        "        save_checkpoint(resnet50, optimizer, epoch, val_loss, val_accuracy, best_checkpoint_path)\n",
        "\n",
        " print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        " # Save the last checkpoint after the final epoch\n",
        " if epoch == num_epochs - 1:\n",
        "        last_checkpoint_path = f'/content/drive/My Drive/checkpoints/last_checkpoint0_5epochs.pth'\n",
        "        save_checkpoint(resnet50, optimizer, epoch, val_loss, val_accuracy, last_checkpoint_path)\n",
        "print('Training finished.')"
      ],
      "metadata": {
        "id": "GqbbKV1_VzsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0bGZab1CmZs",
        "outputId": "5dd4a247-b20e-4d6f-daa9-3d5473577454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8800\n",
            "F1 Score: 0.0506\n",
            "Precision: 1.0000\n",
            "Recall: 0.0260\n",
            "ROC AUC Score: 0.512987012987013\n",
            "Confusion Matrix:\n",
            "[[1096    0]\n",
            " [ 150    4]]\n"
          ]
        }
      ],
      "source": [
        "#testing ERM model on CELEBA test set\n",
        "resnet50.load_state_dict(torch.load(checkpoint_path_best )['model_state_dict']) #loading the model parameters saved in the best checkpoint\n",
        "resnet50.eval() #setting the model to evaluation mode\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): #disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  #moving inputs and labels to device\n",
        "        outputs = resnet50(inputs) #applying the model to the inputs\n",
        "        _, predicted = torch.max(outputs, 1) #predictions\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#computing evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions)\n",
        "precision = precision_score(all_labels, all_predictions)\n",
        "recall = recall_score(all_labels, all_predictions)\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "try:\n",
        "  roc_auc = roc_auc_score(all_labels, all_predictions)\n",
        "except ValueError:\n",
        "  roc_auc = \"Not applicable for multiclass\"\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TFxOBaj6f8b"
      },
      "outputs": [],
      "source": [
        "#directory to masked data\n",
        "masked_data_dir0_5epochs = '/content/drive/My Drive/masked_data0_5epochs/'\n",
        "os.makedirs(masked_data_dir0_5epochs, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading best checkpoint\n",
        "resnet50.load_state_dict(torch.load(checkpoint_path_best, map_location=device)['model_state_dict'])"
      ],
      "metadata": {
        "id": "2ZZ-4U-HWVLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpaGSjpHWAtQ"
      },
      "outputs": [],
      "source": [
        "#definition of XGradCAM heat map generator to the last layer\n",
        "heat_map_generator = XGradCAM(model=resnet50, target_layers=[resnet50.layer4[-1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzyscDUVnqTJ"
      },
      "outputs": [],
      "source": [
        "#applying the correct (the ones needed by mask tune) transformations to the train set and saving data in a list\n",
        "dir = '/content/data/img_align_celeba/img_align_celeba/'\n",
        "mask_set = []\n",
        "for el in trainset_celeba:\n",
        "  img_path = dir+el[0]\n",
        "  target = el[1]\n",
        "  img = Image.open(img_path)\n",
        "  img = t_test(img)\n",
        "  mask_set.append((img, masked_data_dir0_5epochs, img_path, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-9NLKt7-Xby"
      },
      "outputs": [],
      "source": [
        "#saving the mask set list to the specified directory\n",
        "trainset_celeba_to_mask = '/content/drive/My Drive/trainset_celeba_to_mask/'\n",
        "os.makedirs(trainset_celeba_to_mask, exist_ok=True)\n",
        "torch.save(mask_set, trainset_celeba_to_mask +'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q0G7LxRAcRx"
      },
      "outputs": [],
      "source": [
        "#creation of the mask data loader\n",
        "mask_loader = torch.utils.data.DataLoader(CustomDataset(trainset_celeba_to_mask+'tensors_list.pth'), batch_size=8, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applying MaskTune to the data in mask_loader\n",
        "for data in mask_loader:\n",
        "  images, save_dir, images_pathes, targets = data[0], data[1], data[2], data[3]\n",
        "  images = images.to(device)\n",
        "  mask_tune(images, save_dir, images_pathes, targets)"
      ],
      "metadata": {
        "id": "wyhpH21K86NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doefEKI03yLS"
      },
      "outputs": [],
      "source": [
        "#definition of the directories and corresponding lists where to save masked data of classes 0 and 1\n",
        "dir0 ='/content/drive/My Drive/masked_data0_5epochs/0/'\n",
        "dir1 ='/content/drive/My Drive/masked_data0_5epochs/1/'\n",
        "dir0list = os.listdir(dir0)\n",
        "dir1list = os.listdir(dir1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWq37pUa3NbN"
      },
      "outputs": [],
      "source": [
        "#for each element in class 0 of the masked data we apply the transformation taken from the paper\n",
        "mask_data_transformed0 = []\n",
        "\n",
        "for el in dir0list:\n",
        "  target = 0\n",
        "  img = Image.open(dir0+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed0.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C78wPuAUpZj"
      },
      "outputs": [],
      "source": [
        "#for each element in class 1 of the masked data we apply the transformation taken from the paper\n",
        "mask_data_transformed1 = []\n",
        "\n",
        "for el in dir1list:\n",
        "  target = 1\n",
        "  img = Image.open(dir1+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed1.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27Cd6pir_xoi"
      },
      "outputs": [],
      "source": [
        "#summing the two lists containing the masked data of the two classes and saving the resulting list\n",
        "mask_data_transformed=mask_data_transformed0+mask_data_transformed1\n",
        "torch.save(mask_data_transformed, dataset_celeba0_5epochs_transformed_mask+'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWiUDgKmL7a_"
      },
      "outputs": [],
      "source": [
        "#definition of the mask tranformed data loader\n",
        "transformed_data0_5epochs_mask_path = dataset_celeba0_5epochs_transformed_mask+'tensors_list.pth'\n",
        "mask_transformed_loader = torch.utils.data.DataLoader(CustomDataset(transformed_data0_5epochs_mask_path), batch_size=8, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgrhKHkz2rlP",
        "outputId": "6e9cbb2b-be23-44be-f4d6-cbca79fed106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 0.3151\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "#training the Mask tune model on the new masked training dataset\n",
        "scaler = GradScaler() # Initialize the gradient scaler (helps with faster computations)\n",
        "loss = nn.CrossEntropyLoss() #loss\n",
        "resnet50.load_state_dict(torch.load(checkpoint_path_last)['model_state_dict']) #loading the last checkpoint\n",
        "optimizer = optim.SGD(resnet50.parameters(), lr=1e-4, momentum = 0.9, weight_decay = 1e-4) #SGD optimizer\n",
        "\n",
        "num_epochs = 1\n",
        "best_accuracy = 0.0\n",
        "\n",
        "resnet50.train() # Set the model to training mode\n",
        "batch_count = 0\n",
        "running_loss = 0.0\n",
        "for batch_idx, (inputs, labels) in enumerate(mask_transformed_loader):\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device) # Move the inputs and labels to device\n",
        "        batch_count += 1\n",
        "        optimizer.zero_grad() #Reset the gradients of all model parameters to zero before backpropagation\n",
        "\n",
        "        with autocast(enabled=torch.cuda.is_available()): # Use automatic mixed precision if CUDA is available for faster computations and memory savings\n",
        "         outputs = resnet50(inputs) # Forward pass: compute model predictions\n",
        "         loss_f = loss(outputs, labels) # Compute the loss using the predictions and true labels\n",
        "        scaler.scale(loss_f).backward() # Backward pass: scale the loss and compute gradients\n",
        "        scaler.step(optimizer)  # optimizer step\n",
        "        scaler.update()\n",
        "        running_loss += loss_f.item() * inputs.size(0)\n",
        "\n",
        "epoch_loss = running_loss / len(mask_transformed_loader.dataset) # Calculate the average loss for the epoch\n",
        "print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "if epoch == num_epochs - 1:\n",
        "        last_checkpoint_path = f'/content/drive/My Drive/checkpoints/last_checkpoint_mask0_5epochs.pth'\n",
        "        torch.save(resnet50.state_dict(), last_checkpoint_path) # Save the last checkpoint after the final epoch\n",
        "print('Training finished.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1YFJahq5b3n"
      },
      "outputs": [],
      "source": [
        "#path to the last chekpoint\n",
        "checkpoint_masktune='/content/drive/My Drive/checkpoints/last_checkpoint_mask0_5epochs.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BieTFMhl3MHl",
        "outputId": "8175539f-b98a-477e-f578-8dd7535a87e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8960\n",
            "F1 Score: 0.3011\n",
            "Precision: 0.8750\n",
            "Recall: 0.1818\n",
            "ROC AUC Score: 0.5890842733908428\n",
            "Confusion Matrix:\n",
            "[[1092    4]\n",
            " [ 126   28]]\n"
          ]
        }
      ],
      "source": [
        "#testing mask tune\n",
        "resnet50.load_state_dict(torch.load(checkpoint_masktune)) #loading the last checkpoint\n",
        "resnet50.eval() #setting the model to evaluation mode\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():  #disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  #moving inputs and labels to device\n",
        "        outputs = resnet50(inputs)  #applying the model to the inputs\n",
        "        _, predicted = torch.max(outputs, 1) #predictions\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#computing evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions)\n",
        "precision = precision_score(all_labels, all_predictions)\n",
        "recall = recall_score(all_labels, all_predictions)\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "try:\n",
        "  roc_auc = roc_auc_score(all_labels, all_predictions)\n",
        "except ValueError:\n",
        "  roc_auc = \"Not applicable for multiclass\"\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmyDCBFp7jnX"
      },
      "source": [
        "**SELECTIVE CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR-10\n",
        "\n",
        "*   Dataset: The CIFAR-10 dataset consists of 60000 labeled color images, each with a resolution of 32x32 pixels. These images are evenly distributed across 10 different classes, with 6000 images per class. The classes represent everyday objects and animals, including airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The dataset is divided into a training set and a test set, with 50000 images allocated for training and 10000 images for testing.\n",
        "*   Preprocessing: Because of RAM issues, we use only the first 5 classes. We split the training data into 20000 samples for the training set and 5000 for the validation set, and take the first 5 classess of the testing set (5000 samples). For the training set, we perform some transformations which include a random horizontal flip, which helps augment the dataset by mirroring images, thereby increasing variability and reducing overfitting; a random affine transformation, allowing for slight rotations of up to 15 degrees and translations of up to 10% in both the horizontal and vertical directions (this further enhances the model's robustness by providing it with slightly altered versions of the original images); all transformed images are then converted to tensors. In contrast, on the testing and validation sets we apply a simpler transformation that only converts images to tensors without any augmentation. This ensures that the model is evaluated on the original image data, providing a fair assessment of its performance.\n",
        "*   Architecture: The ResNet32 architecture consists of an initial convolutional layer followed by three layers of 5 Basic Blocks each (residual blocks). Each Basic Block contains two convolutional layers with batch normalization and ReLU activation functions. The downsampling is managed through a combination of striding in the convolutions and a LambdaLayer for adjusting the dimensions of the input when necessary. In terms of the training process, the model is trained for 50 epochs using stochastic gradient descent (SGD) as the optimizer, with a learning rate of 0.1, momentum set to 0.9, and a weight decay of 1e-4 to mitigate overfitting. A multi-step learning rate scheduler is employed, reducing the learning rate by half at the 25th and 50th epochs to enhance convergence. During training, the cross-entropy loss function is used to measure the difference between the predicted and actual labels. At the end of each epoch, the modelâ€™s performance is evaluated on a validation set, where both the validation loss and accuracy are computed. The best accuracy achieved during training is tracked, and checkpoints of the model are saved for the best-performing configuration. Additionally, the final probabilities from the validation set are saved for further analysis (selective classification), including computing a threshold to discard the lowest 10% of predicted probabilities.\n",
        "*   Results: we evaluate the performance of the ERM and MaskTune models under both selective classification and non-selective classification conditions. First, the ERM model achieves an accuracy of 0.8032, with an F1 Score of 0.8022, precision of 0.8026, and recall of 0.8032. The confusion matrix reveals that the model performs relatively well, particularly for classes 0, 1, and 4, but struggles with classes 2 and 3. When applying selective classification, the model demonstrates a significant performance improvement, achieving an accuracy of 0.8665, an F1 Score of 0.8639, precision of 0.8649, and recall of 0.8638. The confusion matrix indicates that the selective classification approach helps reducing misclassifications across various classes, particularly in classes 2 and 3, when compared to the non-selective scenario. Subsequently, we test the MaskTune model without selective classification, which reaches an accuracy of 0.8104, with an F1 Score of 0.8071, precision of 0.8100, and recall of 0.8104. The confusion matrix indicates that while the model maintains good performance, it still exhibits some challenges in classifying certain instances, especially in classes 2 and 3. In the final evaluation of the MaskTune model with selective classification, the accuracy improves to 0.9159, with an F1 Score of 0.9129, precision of 0.9134, and recall of 0.9126. The confusion matrix shows a substantial reduction in misclassifications across all classes, highlighting the effectiveness of combining mask tuning with selective classification. Overall, the results demonstrate that both selective classification and mask tuning significantly enhance the model's performance on this dataset."
      ],
      "metadata": {
        "id": "xtkZWGMjtroH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kQ2bliyGIV-"
      },
      "outputs": [],
      "source": [
        "#defining the needed transformations\n",
        "t_train=transforms.Compose([ #data augmentation\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(), #image to tensor\n",
        "    ])\n",
        "\n",
        "t_test=transforms.Compose([transforms.ToTensor()]) #image to tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the CIFAR10 dataset from torchvision according to the train and test split (validation set will be extracted from the train set but with a different transformation)\n",
        "total_trainset_CIFAR10= CIFAR10(root='./data', train=True, download=True, transform=t_train)\n",
        "total_valset_CIFAR10= CIFAR10(root='./data', train=True, download=True, transform=t_test)\n",
        "testset_CIFAR10= CIFAR10(root='./data', train=False, download=True, transform=t_test)"
      ],
      "metadata": {
        "id": "_QM78v1RGZY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5WMM8LouLm0"
      },
      "outputs": [],
      "source": [
        "#we use only 5 classes because of time and RAM issues and extract them with the filter_classes function\n",
        "trainset_CIFAR10_subset = filter_classes(total_trainset_CIFAR10)\n",
        "valset_CIFAR10_subset = filter_classes(total_valset_CIFAR10)\n",
        "testset_CIFAR10_subset = filter_classes(testset_CIFAR10)\n",
        "\n",
        "#converting the subsets to lists\n",
        "trainset_CIFAR10_subset=list(trainset_CIFAR10_subset)\n",
        "valset_CIFAR10_subset=list(valset_CIFAR10_subset)\n",
        "testset_CIFAR10_subset=list(testset_CIFAR10_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhyMueRQgsDb"
      },
      "outputs": [],
      "source": [
        "#saving the list containg the new test set\n",
        "testset_CIFAR10_path = '/content/drive/My Drive/testset_CIFAR10_subset/'\n",
        "os.makedirs(testset_CIFAR10_path, exist_ok=True)\n",
        "torch.save(testset_CIFAR10_subset, testset_CIFAR10_path + 'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j86lFVHFQl_L"
      },
      "outputs": [],
      "source": [
        "#we take 5000 random samples from the training set and save the indices on google drive in order to retrieve always the same images to create the validation set\n",
        "valset_CIFAR10_indices=random.sample(range(len(trainset_CIFAR10_subset)), 5000)\n",
        "valset_CIFAR10_indices_path='/content/drive/MyDrive/valset_CIFAR10_subset_indices.pkl' # Define the path to the file where the CIFAR10 indices were previously saved.\n",
        "with open(valset_CIFAR10_indices_path, 'wb') as f:\n",
        "    pickle.dump(valset_CIFAR10_indices, f) #serialize the 'valset_CIFAR10_indices' object and save it to the file 'f'\n",
        "\n",
        "with open(valset_CIFAR10_indices_path, 'rb') as f:\n",
        "    valset_CIFAR10_indices = pickle.load(f) #deserialize the data from the file 'f'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvx2D8MZS23x"
      },
      "outputs": [],
      "source": [
        "#we dropped the images corresponding to the validation indices to obtain the train set\n",
        "trainset_CIFAR10 = [trainset_CIFAR10_subset[i] for i in range(len(trainset_CIFAR10_subset)) if i not in valset_CIFAR10_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSMz7RdIf82T"
      },
      "outputs": [],
      "source": [
        "#saving the list containing the train tensors\n",
        "trainset_CIFAR10_path = '/content/drive/My Drive/trainset_CIFAR10_subset/'\n",
        "os.makedirs(trainset_CIFAR10_path, exist_ok=True)\n",
        "torch.save(trainset_CIFAR10, trainset_CIFAR10_path + 'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUL_qnDyQdEj"
      },
      "outputs": [],
      "source": [
        "#selecting the elements of the valset subset corresponding to the indices defined before\n",
        "valset_CIFAR10= [valset_CIFAR10_subset[i] for i in valset_CIFAR10_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_jG1wx1hODD"
      },
      "outputs": [],
      "source": [
        "#saving the list containing the validation tensors\n",
        "valset_CIFAR10_path = '/content/drive/My Drive/valset_CIFAR10_subset/'\n",
        "os.makedirs(valset_CIFAR10_path, exist_ok=True)\n",
        "torch.save(valset_CIFAR10, valset_CIFAR10_path + 'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwm6Jd4aYVhc"
      },
      "outputs": [],
      "source": [
        "#we did the same thing as for the training set but we used the original set transformed with t_test\n",
        "trainset_CIFAR10_to_mask=[valset_CIFAR10_subset[i] for i in range(len(valset_CIFAR10_subset)) if i not in valset_CIFAR10_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KuDl15ehrDm"
      },
      "outputs": [],
      "source": [
        "#saving the list containing the data to mask tensors\n",
        "trainset_CIFAR10_to_mask_path = '/content/drive/My Drive/trainset_CIFAR10_to_mask_subset/'\n",
        "os.makedirs(trainset_CIFAR10_to_mask_path, exist_ok=True)\n",
        "torch.save(trainset_CIFAR10_to_mask, trainset_CIFAR10_to_mask_path + 'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEaydbTqUTv-"
      },
      "outputs": [],
      "source": [
        "#defining the train, validatation, test and data to mask data loaders\n",
        "train_loader_CIFAR10=torch.utils.data.DataLoader(CustomDataset(trainset_CIFAR10_path+'tensors_list.pth'), batch_size=128, shuffle=True, num_workers=8)\n",
        "val_loader_CIFAR10=torch.utils.data.DataLoader(CustomDataset(valset_CIFAR10_path+'tensors_list.pth'), batch_size=128, shuffle=False, num_workers=8)\n",
        "test_loader_CIFAR10=torch.utils.data.DataLoader(CustomDataset(testset_CIFAR10_path+'tensors_list.pth'), batch_size=128, shuffle=False, num_workers=8)\n",
        "train_loader_to_mask_CIFAR10=torch.utils.data.DataLoader(CustomDataset(trainset_CIFAR10_to_mask_path+'tensors_list.pth'), batch_size=128, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVA7aakrovm2"
      },
      "outputs": [],
      "source": [
        "#directories to save chekpoints and probabilities of the selected class\n",
        "checkpoints_CIFAR10_path = '/content/drive/My Drive/checkpoints_CIFAR10/'\n",
        "os.makedirs(checkpoints_CIFAR10_path, exist_ok=True)\n",
        "probabilities_CIFAR10_path = '/content/drive/My Drive/probabilities_CIFAR10/'\n",
        "os.makedirs(probabilities_CIFAR10_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training ERM model with the CIFAR10 training set for 50 epoch instead of 300\n",
        "model = ResNet32(num_classes=5).to(device) #move the model to device\n",
        "loss = nn.CrossEntropyLoss() #loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum = 0.9, weight_decay = 1e-4) #SGD optimizer\n",
        "lr_step = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50], gamma=0.5) #learning rate scheduler\n",
        "best_accuracy = -math.inf #initialize the best accuracy tracker\n",
        "\n",
        "num_epochs=50\n",
        "final_selected_probs = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() #setting the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader_CIFAR10:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving inputs and labels to device\n",
        "        optimizer.zero_grad() #reset the gradients to 0 before backpropagation\n",
        "        outputs = model(inputs) #applying the model to the inputs\n",
        "        loss_f = loss(outputs, labels)\n",
        "        loss_f.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_f.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader_CIFAR10.dataset) #calculate the average loss for the epoch\n",
        "    val_loss, val_accuracy, val_probs = validate_selective_classification(model, val_loader_CIFAR10, loss) #validation loss, validation accuracy and probabilities of the selected class\n",
        "    if epoch == num_epochs - 1: #we use the probabilities of the last epoch\n",
        "          final_selected_probs = val_probs\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "    lr_step.step()\n",
        "\n",
        "    #save the model checkpoint if the validation accuracy improves\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_checkpoint_path = f'/content/drive/My Drive/checkpoints_CIFAR10/best_checkpoint_5classes50epochs.pth'\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, val_accuracy, best_checkpoint_path)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    #save the last checkpoint\n",
        "    if epoch == num_epochs - 1:\n",
        "        last_checkpoint_path = f'/content/drive/My Drive/checkpoints_CIFAR10/last_checkpoint_5classes50epochs.pth'\n",
        "        save_checkpoint(model, optimizer, epoch, val_loss, val_accuracy, last_checkpoint_path)\n",
        "\n",
        "#saving the probabilities of the selected class to a csv file\n",
        "if final_selected_probs is not None:\n",
        "        df = pd.DataFrame(final_selected_probs, columns=['Selected_Prob'])\n",
        "        output_path = os.path.join(probabilities_CIFAR10_path, 'final_probabilities_5classes50epochs.csv')\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f'Final epoch probabilities saved to: {output_path}')\n",
        "\n",
        "#computing the threshold to drop the 10% with the lowest probabilities\n",
        "sorted_probs = np.sort(final_selected_probs)\n",
        "threshold = sorted_probs[int(0.1 * len(sorted_probs))]\n",
        "print(f'Calculated threshold: {threshold}')"
      ],
      "metadata": {
        "id": "rOk8g78abF3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kunfp_cPpUB2"
      },
      "outputs": [],
      "source": [
        "model = ResNet32(num_classes=5).to(device) #moving the model to device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4hnFJwXHVeb"
      },
      "outputs": [],
      "source": [
        "#paths to last and best checkpoints\n",
        "last_checkpoint_path = '/content/drive/My Drive/checkpoints_CIFAR10/last_checkpoint_5classes50epochs.pth'\n",
        "best_checkpoint_path = '/content/drive/My Drive/checkpoints_CIFAR10/best_checkpoint_5classes50epochs.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4co4vEmGHoTV"
      },
      "outputs": [],
      "source": [
        "#reading the csv probabilities file, selecting the column containing probabilities and flattening it to a list\n",
        "probabilities_df = pd.read_csv('/content/drive/My Drive/probabilities_CIFAR10/final_probabilities_5classes50epochs.csv')\n",
        "probabilities = probabilities_df['Selected_Prob'].values.flatten()\n",
        "#sorting the list\n",
        "sorted=np.sort(probabilities)\n",
        "threshold = sorted[int(0.1 * len(sorted))] #computing the threshold to drop the 10% with the lowest probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZc8CWOnotcL",
        "outputId": "0b0abcc0-2b49-4872-fc76-d3a029968a6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8032\n",
            "F1 Score: 0.8022\n",
            "Precision: 0.8026\n",
            "Recall: 0.8032\n",
            "ROC AUC Score: Not applicable for multiclass\n",
            "Confusion Matrix:\n",
            "[[868  39  56  23  14]\n",
            " [ 21 956   7  13   3]\n",
            " [ 75   8 730  87 100]\n",
            " [ 49  20 144 701  86]\n",
            " [ 43   7 109  80 761]]\n"
          ]
        }
      ],
      "source": [
        "# testing ERM model\n",
        "model.load_state_dict(torch.load(best_checkpoint_path )['model_state_dict']) #loading the best checkpoint\n",
        "model.eval() #setting the model to evaluation mode\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "\n",
        "with torch.no_grad(): #disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader_CIFAR10:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving inputs and labels to device\n",
        "\n",
        "        outputs = model(inputs) #applying the model to the inputs\n",
        "        _, predicted = torch.max(outputs, 1) #predictions\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#computing evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "try:\n",
        "  roc_auc = roc_auc_score(all_labels, all_predictions)\n",
        "except ValueError:\n",
        "  roc_auc = \"Not applicable for multiclass\"\n",
        "\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing ERM model dropping 10% of samples\n",
        "model.load_state_dict(torch.load(best_checkpoint_path )['model_state_dict']) #loading the best checkpoint\n",
        "model.eval() #setting the model to evaluation mode\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "\n",
        "with torch.no_grad(): #disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader_CIFAR10:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving inputs and labels to device\n",
        "\n",
        "        outputs = model(inputs) #applying the model to the inputs\n",
        "        probs = F.softmax(outputs, dim=1)  #computing the probabilities\n",
        "        selected_probs = probs.gather(1, labels.view(-1, 1)).cpu().numpy() #extract the predicted probability for the true class for each sample.\n",
        "        selected_probs=np.array(selected_probs).flatten() # Convertion to a 1D NumPy array and flattening\n",
        "        _, predicted = torch.max(probs, 1) #predictions\n",
        "\n",
        "\n",
        "        mask = selected_probs > threshold # Create a boolean mask where 'selected_probs' are greater than the specified 'threshold'\n",
        "        filtered_predictions = predicted[mask] # Filter the 'predicted' array using the mask to keep only the predictions where the condition is true\n",
        "        filtered_labels = labels[mask] # Filter the 'labels' array using the same mask to keep only the corresponding labels\n",
        "\n",
        "        print(f'Batch size: {inputs.size(0)}')\n",
        "        print(f'Mask shape: {mask.shape}, Number of filtered predictions: {filtered_predictions.size(0)}')\n",
        "\n",
        "        all_predictions.extend(filtered_predictions.cpu().numpy())\n",
        "        all_labels.extend(filtered_labels.cpu().numpy())\n",
        "\n",
        "#computing the evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "try:\n",
        "  roc_auc = roc_auc_score(all_labels, all_predictions)\n",
        "except ValueError:\n",
        "  roc_auc = \"Not applicable for multiclass\"\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "-3k7S2G_a_yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(best_checkpoint_path )['model_state_dict']) #loading the best checkpoint"
      ],
      "metadata": {
        "id": "HlYvTr3wbCOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdGMONscrOqM"
      },
      "outputs": [],
      "source": [
        "heat_map_generator = XGradCAM(model=model, target_layers=[model.get_grad_cam_target_layer()]) #creating the XGradCAM heatmap generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVme2Y_43JLq"
      },
      "outputs": [],
      "source": [
        "#path to the dataset to mask\n",
        "CIFAR10_dataset_to_mask_path = '/content/drive/My Drive/CIFAR10datasettomask_5classes/'\n",
        "os.makedirs(CIFAR10_dataset_to_mask_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R4vFd9OBL4s"
      },
      "outputs": [],
      "source": [
        "#we saved the trainset to mask on google drive after applying the ToPILImage transformation\n",
        "i=0\n",
        "for el in trainset_CIFAR10_to_mask:\n",
        "  image=el[0]\n",
        "  t=transforms.ToPILImage()\n",
        "  t(image).save(CIFAR10_dataset_to_mask_path+str(i)+'.jpg')\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aBVtm3ht3oL"
      },
      "outputs": [],
      "source": [
        "#directory to save the masked data\n",
        "masked_data_dir_CIFAR10 = '/content/drive/My Drive/maskeddatasetCIFAR10_5classes50epochs/'\n",
        "os.makedirs(masked_data_dir_CIFAR10, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GzuhA-ysIWp"
      },
      "outputs": [],
      "source": [
        "#creation of a list with the correct format to be given in input to the MaskTune function later\n",
        "CIFAR10_list=[]\n",
        "for i in range(len(trainset_CIFAR10_to_mask)):\n",
        "  CIFAR10_list.append((trainset_CIFAR10_to_mask[i][0],masked_data_dir_CIFAR10, CIFAR10_dataset_to_mask_path+str(i)+'.jpg', trainset_CIFAR10_to_mask[i][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLgyUqrPJPPs"
      },
      "outputs": [],
      "source": [
        "#definition of the mask data loader\n",
        "mask_loader_CIFAR10 = DataLoader(CIFAR10_list, batch_size=128, shuffle=False, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#computing the mask_tune function for the mask data loader\n",
        "for data in mask_loader_CIFAR10:\n",
        "  images, save_dir, images_pathes, targets = data[0], data[1], data[2], data[3]\n",
        "  images = images.to(device)\n",
        "  mask_tune(images, save_dir, images_pathes, targets)"
      ],
      "metadata": {
        "id": "DY-9WtiwFiXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCw_rx66RkLY"
      },
      "outputs": [],
      "source": [
        "#definition of the directories where the masked data of the 5 classes are saved and corresponding lists\n",
        "dir0 ='/content/drive/My Drive/maskeddatasetCIFAR10_5classes50epochs/0/'\n",
        "dir1 ='/content/drive/My Drive/maskeddatasetCIFAR10_5classes50epochs/1/'\n",
        "dir2 ='/content/drive/My Drive/maskeddatasetCIFAR10_5classes50epochs/2/'\n",
        "dir3 ='/content/drive/My Drive/maskeddatasetCIFAR10_5classes50epochs/3/'\n",
        "dir4 ='/content/drive/My Drive/maskeddatasetCIFAR10_5classes50epochs/4/'\n",
        "\n",
        "dir0list = os.listdir(dir0)\n",
        "dir1list = os.listdir(dir1)\n",
        "dir2list = os.listdir(dir2)\n",
        "dir3list = os.listdir(dir3)\n",
        "dir4list = os.listdir(dir4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqYzyPE4bCxo"
      },
      "outputs": [],
      "source": [
        "#applying transformations to the masked data of class 0\n",
        "mask_data_transformed0 = []\n",
        "\n",
        "for el in dir0list:\n",
        "  target = 0\n",
        "  img = Image.open(dir0+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed0.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzbPdan5SyiR"
      },
      "outputs": [],
      "source": [
        "#applying transformations to the masked data of class 1\n",
        "mask_data_transformed1 = []\n",
        "\n",
        "for el in dir1list:\n",
        "  target = 1\n",
        "  img = Image.open(dir1+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed1.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kI2QCo9HTAcB"
      },
      "outputs": [],
      "source": [
        "#applying transformations to the masked data of class 2\n",
        "mask_data_transformed2 = []\n",
        "\n",
        "for el in dir2list:\n",
        "  target = 2\n",
        "  img = Image.open(dir2+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed2.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XZvFll4sTBnS"
      },
      "outputs": [],
      "source": [
        "#applying transformations to the masked data of class 3\n",
        "mask_data_transformed3 = []\n",
        "\n",
        "for el in dir3list:\n",
        "  target = 3\n",
        "  img = Image.open(dir3+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed3.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGdPS7b4TCSZ"
      },
      "outputs": [],
      "source": [
        "#applying transformations to the masked data of class 4\n",
        "mask_data_transformed4 = []\n",
        "\n",
        "for el in dir4list:\n",
        "  target = 4\n",
        "  img = Image.open(dir4+el)\n",
        "  img = t_test(img)\n",
        "  mask_data_transformed4.append((img, target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BwiOkJxT-e_"
      },
      "outputs": [],
      "source": [
        "#summing the 5 lists\n",
        "mask_data_transformed=mask_data_transformed0+mask_data_transformed1+mask_data_transformed2+mask_data_transformed3+mask_data_transformed4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWPqJaKGBQxp"
      },
      "outputs": [],
      "source": [
        "#saving the list of masked data tensors\n",
        "masked_data_transformed_CIFAR10 = '/content/drive/My Drive/maskeddatasettransformedCIFAR10_5classes50epochs/'\n",
        "os.makedirs(masked_data_transformed_CIFAR10, exist_ok=True)\n",
        "torch.save(mask_data_transformed, masked_data_transformed_CIFAR10+'tensors_list.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defition of the mask transformed data loader\n",
        "mask_transformed_loader_CIFAR10 = torch.utils.data.DataLoader(CustomDataset(masked_data_transformed_CIFAR10+'tensors_list.pth'), batch_size=128, shuffle=True, num_workers=8)"
      ],
      "metadata": {
        "id": "pBucx-uEGmpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWBDdZT5QQRl"
      },
      "outputs": [],
      "source": [
        "#directory to the MaskTune probabilities\n",
        "probabilities_CIFAR10_masktune_path = '/content/drive/My Drive/probabilities_masktune_CIFAR10/'\n",
        "os.makedirs(probabilities_CIFAR10_masktune_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training mask tune and saving probabilities\n",
        "loss = nn.CrossEntropyLoss() #loss\n",
        "model = ResNet32(num_classes=5).to(device) #moving the model to device\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum = 0.9, weight_decay = 1e-4) #SGD optimizer\n",
        "model, optimizer, start_epoch, val_loss, accuracy = load_checkpoint(model, optimizer, last_checkpoint_path) #loading the last checkpoint\n",
        "\n",
        "num_epochs=1\n",
        "final_selected_probs = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() #setting the model to train mode\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in mask_transformed_loader_CIFAR10:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving the inputs and labels to device\n",
        "        optimizer.zero_grad() #reset the gradients to 0 before backpropagation\n",
        "        outputs = model(inputs) #applying the model to the inputs\n",
        "        loss_f = loss(outputs, labels)\n",
        "        loss_f.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss_f.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(mask_transformed_loader_CIFAR10.dataset) #calculate the average loss for the epoch\n",
        "    val_loss, val_accuracy, val_probs = validate_selective_classification(model, val_loader_CIFAR10, loss) #validation loss, validation accuracy and probabilities of the selected class\n",
        "    if epoch == num_epochs - 1: #we use the probabilities of the last epoch\n",
        "          final_selected_probs = val_probs\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    #save the last checkpoint\n",
        "    if epoch == num_epochs - 1:\n",
        "        last_checkpoint_path = f'/content/drive/My Drive/checkpoints_CIFAR10/last_checkpoint_mask_CIFAR10_5classes50epochs_prob.pth'\n",
        "        torch.save(model.state_dict(), last_checkpoint_path)\n",
        "\n",
        "#saving the probabilities of the selected class to a csv file\n",
        "if final_selected_probs is not None:\n",
        "        df = pd.DataFrame(final_selected_probs, columns=['Selected_Prob'])\n",
        "        output_path = os.path.join(probabilities_CIFAR10_masktune_path, 'final_probabilities_masktune_5classes50epochs.csv')\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f'Final epoch probabilities saved to: {output_path}')\n",
        "#computing the threshold to drop the 10% with the lowest probabilities\n",
        "sorted_probs = np.sort(final_selected_probs)\n",
        "threshold = sorted_probs[int(0.1 * len(sorted_probs))]\n",
        "print(f'Calculated threshold: {threshold}')"
      ],
      "metadata": {
        "id": "HRqC50Uya5vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path to the last checkpoint\n",
        "checkpoint_masktune_CIFAR10_prob='/content/drive/My Drive/checkpoints_CIFAR10/last_checkpoint_mask_CIFAR10_5classes50epochs_prob.pth'"
      ],
      "metadata": {
        "id": "mn8WV-v6mVOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4KLEpsvpfLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a95ebe5-46de-458d-baa4-37c20f7d8f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8104\n",
            "F1 Score: 0.8071\n",
            "Precision: 0.8100\n",
            "Recall: 0.8104\n",
            "ROC AUC Score: Not applicable for multiclass\n",
            "Confusion Matrix:\n",
            "[[907  31  22  19  21]\n",
            " [ 33 956   0   9   2]\n",
            " [108  17 627 136 112]\n",
            " [ 44  35  68 766  87]\n",
            " [ 41  10  59  94 796]]\n"
          ]
        }
      ],
      "source": [
        "#testing mask tune with no selective classification\n",
        "model.load_state_dict(torch.load(checkpoint_masktune_CIFAR10_prob)) #loading the last checkpoint\n",
        "model.eval() #setting the model to evaluation mode\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():#disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader_CIFAR10:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving inputs and labels to device\n",
        "\n",
        "        outputs = model(inputs) #applying the model to inputs\n",
        "        _, predicted = torch.max(outputs, 1) #predictions\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#computing evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "try:\n",
        "  roc_auc = roc_auc_score(all_labels, all_predictions)\n",
        "except ValueError:\n",
        "  roc_auc = \"Not applicable for multiclass\"\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing mask tune with selective classification\n",
        "model = ResNet32(num_classes=5).to(device) #moving the model to device\n",
        "model.load_state_dict(torch.load(checkpoint_masktune_CIFAR10_prob)) #loading the last checkpoint\n",
        "model.eval() #setting the model to evaluation mode\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():#disabling gradient calculation during testing\n",
        "    for inputs, labels in test_loader_CIFAR10:\n",
        "        inputs, labels = inputs.to(device), labels.to(device) #moving inputs and labels to device\n",
        "\n",
        "        outputs = model(inputs) #applying the model to inputs\n",
        "        probs = F.softmax(outputs, dim=1)  #computing the probabilities\n",
        "        selected_probs = probs.gather(1, labels.view(-1, 1)).cpu().numpy() #extract the predicted probability for the true class for each sample\n",
        "        selected_probs=np.array(selected_probs).flatten() # Convertion to a 1D NumPy array and flattening\n",
        "        _, predicted = torch.max(probs, 1) #predictions\n",
        "\n",
        "        mask = selected_probs > threshold # Create a boolean mask where 'selected_probs' are greater than the specified 'threshold'\n",
        "        filtered_predictions = predicted[mask] # Filter the 'predicted' array using the mask to keep only the predictions where the condition is true\n",
        "        filtered_labels = labels[mask] # Filter the 'labels' array using the same mask to keep only the corresponding labels\n",
        "\n",
        "        print(f'Batch size: {inputs.size(0)}')\n",
        "        print(f'Mask shape: {mask.shape}, Number of filtered predictions: {filtered_predictions.size(0)}')\n",
        "\n",
        "        all_predictions.extend(filtered_predictions.cpu().numpy())\n",
        "        all_labels.extend(filtered_labels.cpu().numpy())\n",
        "\n",
        "#computing evaluation metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "precision = precision_score(all_labels, all_predictions, average='macro')\n",
        "recall = recall_score(all_labels, all_predictions, average='macro')\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "try:\n",
        "  roc_auc = roc_auc_score(all_labels, all_predictions)\n",
        "except ValueError:\n",
        "  roc_auc = \"Not applicable for multiclass\"\n",
        "\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'ROC AUC Score: {roc_auc}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "uCOecrkiaxA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN RESULTS AND FINAL CONSIDERATIONS**\n",
        "\n",
        "The final analysis across different tasks and datasets highlights the effectiveness of MaskTune in improving model performance, particularly in addressing issues related to spurious features and selective classification.\n",
        "\n",
        "In the classification tasks with spurious features, MaskTune demonstrates its capability to significantly enhance model robustness and fairness. On MNIST, MaskTune decreases the accuracy gap between the original and biased test sets, showing a marked improvement in both, which underscores its ability to reduce the model's sensitivity to bias. Similarly, on CelebA, MaskTune not only improves overall accuracy but also achieves a better balance between precision and recall, as evidenced by the improved F1 score and ROC AUC score, indicating enhanced discrimination and a more reliable identification of true positives.\n",
        "\n",
        "In the selective classification task on CIFAR-10, MaskTune again proves its utility. While the ERM model benefits from selective classification, MaskTune further enhances performance, particularly when combined with selective classification. The improvements in accuracy, F1 score, precision, and recall, along with a more favorable confusion matrix, show that MaskTune helps in reducing misclassifications and improving the model's confidence in its predictions.\n",
        "\n",
        "Overall, these results underscore MaskTune's efficiency in addressing spurious correlations and improving model reliability through selective classification, making it a valuable technique for enhancing the performance of deep learning models across various datasets and tasks."
      ],
      "metadata": {
        "id": "mwN4PHFCWJAO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}